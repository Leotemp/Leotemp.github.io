<?xml version="1.0" encoding="utf-8"?>






<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>九月</title>
        <link>https://tmliang.github.io/</link>
        <description>记录我的科研生活</description>
        <generator>Hugo 0.81.0 https://gohugo.io/</generator>
        
            <language>zh-CN</language>
        
        
            <managingEditor>tm.liang@outlook.com (Liang Tianming)</managingEditor>
        
        
            <webMaster>tm.liang@outlook.com (Liang Tianming)</webMaster>
        
        
            <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</copyright>
        
        <lastBuildDate>Wed, 10 Mar 2021 23:53:33 &#43;0800</lastBuildDate>
        
            <atom:link rel="self" type="application/rss&#43;xml" href="https://tmliang.github.io/rss.xml" />
        
        
            <item>
                <title>An Improved Baseline for Sentence-level Relation Extraction</title>
                <link>https://tmliang.github.io/research/an-improved-baseline-for-sentence-level-relation-extraction/</link>
                <guid isPermaLink="true">https://tmliang.github.io/research/an-improved-baseline-for-sentence-level-relation-extraction/</guid>
                <pubDate>Wed, 10 Mar 2021 13:07:52 &#43;0800</pubDate>
                
                    <author>tm.liang@outlook.com (Liang Tianming)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</copyright>
                
                    <description>&lt;p&gt;&lt;strong&gt;An Improved Baseline for Sentence-level Relation Extraction (Arxiv-2021)&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.01373&#34;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文聚焦于当前关系抽取实践中两个非常重要却经常被忽视的部分: &lt;strong&gt;实体的表示方式&lt;/strong&gt;和&lt;strong&gt;NA类的处理&lt;/strong&gt;. 通篇语言流畅, 而且叙述结构非常清晰, 个人认为是今年最好的一篇关系抽取文章.&lt;/p&gt;
&lt;h3 id=&#34;entity-representation&#34;&gt;Entity Representation&lt;/h3&gt;
&lt;p&gt;替换实体对的方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Entity Mask&lt;/strong&gt;: [SUBJ-类型名], [OBJ-类型名]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entity Marker&lt;/strong&gt;: [E1]头实体[/E1], [E2]尾实体[/E2]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entity Marker(punct)&lt;/strong&gt;: @头实体@, #尾实体#&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type Entity Marker&lt;/strong&gt;: [E1-类型名]头实体[/E1-类型名], [E2-类型名]尾实体[/E2-类型名]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type Entity Marker(punct)&lt;/strong&gt;: @*类型名*头实体@, #^类型名^尾实体#&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实验表明后两种表现最好.&lt;/p&gt;
&lt;p&gt;下面是作者关于训练时是否应该掩盖实体名的分析:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;有人认为训练时不掩盖实体名, 会导致测试集的标签泄漏, 而且无法扩展到新实体. 但我认为, 实体名提供了重要的实体信息, 而且, 如果实体名不能使用, 那么那些基于外部三元组知识的方法也不该使用, 因为他们同样泄露了目标实体的信息.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为了证实自己的观点, 作者将测试集中包含训练实体的句子过滤掉, 最终结果表明, 非mask的仍然优于mask.&lt;/p&gt;
&lt;h3 id=&#34;confidence-based-classification&#34;&gt;Confidence-based Classification&lt;/h3&gt;
&lt;p&gt;作者认为, 不应该将NA视作关系分类中的一个独立类别, 因为NA中包含着无数种不同语义的实例, 很难由一个类别建模.&lt;/p&gt;
&lt;p&gt;本方法设立阈值进行分类, 若预测分数低于阈值, 则被视作NA. 类似于Openset Classification和Out-of-distribution Detection.&lt;/p&gt;
&lt;p&gt;Loss的设计:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NA类具有极低的置信度&lt;/li&gt;
&lt;li&gt;正关系具有极高的置信度&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2可以通过Cross Entropy实现. 但对于1, 直接降低置信度是很难优化的, 因为最大预测概率所对应的关系也会被更新, 因此, 提出了最小化NA类的置信度代理:&lt;/p&gt;
&lt;div&gt;
$$\begin{aligned}
    c_{\text {sup }} &amp;=\sum_{r \in \mathcal{R}} \boldsymbol{p}_{r}^{2} \\
    \mathcal{L}_{\text {conf }} &amp;=\log \left(1-c_{\text {sup }}\right)
\end{aligned}$$
&lt;/div&gt;
&lt;p&gt;由计算可知, 置信度$c=\max_{r\in R}p_{r}\leq C_{\text{sup }}$, 最小化$\mathcal{L}_{\text {conf }}$相当于最小化$c$, 使得训练更稳定.&lt;/p&gt;
&lt;p&gt;对关系$r$的logit值$l_r$求导:&lt;/p&gt;
&lt;div&gt;
$$\frac{\partial \mathcal{L}_{\mathrm{conf}}}{l_{r}}=-\frac{2 \boldsymbol{p}_{r}\left(\boldsymbol{p}_{r}-\sum_{r \in \mathcal{R}} \boldsymbol{p}_{r}^{2}\right)}{1-\sum_{r \in \mathcal{R}} \boldsymbol{p}_{r}^{2}}$$
&lt;/div&gt;
&lt;p&gt;由此可知:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当$\boldsymbol{p}_{r}=\frac{1}{\mid \mathcal{R}]}$, $\forall r \in \mathcal{R}$, $\mathcal{L}_{\text {conf }}$最小, 即此时各关系为均匀分布.&lt;/li&gt;
&lt;li&gt;训练实例的$\mathcal{L}_{\text {conf }}$惩罚为$\frac{1}{1-\sum_{r \in \mathcal{R}} \boldsymbol{p}_{r}^{2}}$, 因此置信度高的实例会受到更多惩罚.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最终, 结合&lt;strong&gt;Type Entity Marker(punct)&lt;strong&gt;和&lt;/strong&gt;Confidence-based Classification&lt;/strong&gt;的方法在TACRED和SemEval 2010 Task 8上都取得了SOTA结果.&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://tmliang.github.io/tags/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/">关系抽取</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>基于远程监督的关系抽取研究现状</title>
                <link>https://tmliang.github.io/research/%E5%9F%BA%E4%BA%8E%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3%E7%9A%84%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%A0%94%E7%A9%B6%E7%8E%B0%E7%8A%B6/</link>
                <guid isPermaLink="true">https://tmliang.github.io/research/%E5%9F%BA%E4%BA%8E%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3%E7%9A%84%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%A0%94%E7%A9%B6%E7%8E%B0%E7%8A%B6/</guid>
                <pubDate>Sun, 09 Feb 2020 18:25:58 &#43;0800</pubDate>
                
                    <author>tm.liang@outlook.com (Liang Tianming)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</copyright>
                
                    <description>&lt;p&gt;关系抽取建立在命名实体识别的基础上，是知识图谱补全的重要任务之一。关系抽取的目标是从文本中提取出两个实体之间的语义关系。传统的关系抽取虽然已经达到相当好的效果，但需要人工预先精准地标注大量句子级别的数据，标注成本十分昂贵。在实际场景中，面向数以千计的关系、数以千万计的实体对、以及数以亿计的句子，依靠人工标注训练数据几乎是不可能完成的任务&lt;sup&gt;[1]&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;为了获得大量的机器标注的训练数据，Mintz M&lt;sup&gt;[2]&lt;/sup&gt;等人将远程监督的概念应用到关系抽取任务中，并提出一个非常著名的假设：两个实体如果在知识库中存在某种关系，则包含该两个实体的所有句子都在以某种方式表达这种关系。远程监督能够显著减少关系抽取任务所需的标注成本，但其过强的假设为数据引入了大量噪声。自2009年远程监督的假设提出之后，便有大量的学者投身于此研究中，致力于降低远程监督所带来的标签噪声。&lt;/p&gt;
&lt;p&gt;远程监督的第一个重大进展是多实例学习的引入。Zeng&lt;sup&gt;[3]&lt;/sup&gt;等人弱化了远程监督的假设：两个实体如果在知识库中存在某种关系，则包含该两个实体的所有句子中，至少有一句在表达这个关系。他们将包含同一对实体的所有句子构成一个包，然后使用PCNN模型提取每个句子的表征，最后以包中概率最大的句子表征作为这个包的表征，通过Softmax进行分类。Lin&lt;sup&gt;[4]&lt;/sup&gt;等人则认为只使用包中最大概率的句子进行分类，会丢弃掉包中其他句子的有效信息，于是他们引入了句子级别的注意力，为包中每个句子赋予一个注意力权重，然后以包中所有句子表征的加权和作为这个包的表征。Lin等人的方法能够非常有效地降低远程监督所带来的标签噪声，现已成为基于远程监督的关系抽取中常用的Baseline了，许多学者在此基础上深入研究。&lt;/p&gt;
&lt;p&gt;另一种降低标签噪声影响的思路是提高模型的抗噪能力。Wu&lt;sup&gt;[5]&lt;/sup&gt;等人引入了对抗训练，以此增强模型对噪声数据的抵抗能力。&lt;/p&gt;
&lt;p&gt;虽然多实例学习和对抗训练可以降低标签噪声的影响，但并不能从根本上消除噪声。Feng&lt;sup&gt;[6]&lt;/sup&gt;等人借助强化学习模型，直接过滤掉训练样本中的噪声句子，然后使用剩下的训练数据来训练模型。Qin&lt;sup&gt;[7]&lt;/sup&gt;等人使用了生成对抗网络。他们使用生成器来划分数据，使用判别器来评价该划分的好坏，通过生成器和判别器的相互促进，最终得到一个能从训练集中筛选出噪声句子的生成器。&lt;/p&gt;
&lt;p&gt;上述工作虽然都在一定程度上降低了远程监督所带来的噪声影响，但仍局限于语料集本身，可用的信息只有文本统计的数据。近年来许多学者开始尝试借助外部知识的指导，进一步提高关系抽取模型的能力。Lei&lt;sup&gt;[8]&lt;/sup&gt;等人提出了协同降噪框架，利用知识图谱中的监督信息来降低远程监督的标签噪声。该框架使用两个神经网络分别在文本和知识图谱上学习，然后通过双向知识蒸馏模块完成它们之间的共同学习。Vashishth&lt;sup&gt;[9]&lt;/sup&gt;等人使用关系别名和实体类型这两种知识库信息来丰富了文本中的关系和实体信息。Xu&lt;sup&gt;[10]&lt;/sup&gt;等人通过知识图谱嵌入和关系抽取的联合学习，显著地提高了在远程监督下的关系抽取性能。&lt;/p&gt;
&lt;p&gt;除了借助外部知识的指导，还有学者在研究如何借助有监督下的关系抽取模型来指导远程监督下的关系抽取。然而现有的有监督数据集和远程监督数据集中的关系标签通常是不重合的，如何结合两种数据以提高远程监督下的关系抽取性能，是这个方向的首要任务。大多数的结合方法都是把两个数据集简单的混到一起，然后增加分类的类别。Beltagy&lt;sup&gt;[11]&lt;/sup&gt;等人则提出了更有效的结合方法，他们在有监督数据集上训练了一个二元关系分类器，判断句子是否表达了两个实体之间的关系，然后用此分类器来滤除远程监督样本中的假阳性噪声数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]   韩旭, 高天宇, 刘知远, et al. 知识图谱从哪里来：实体关系抽取的现状与未来[EB/OL]. 2019-11-7[2020-1-9]. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/91762831&#34;&gt;https://zhuanlan.zhihu.com/p/91762831&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[2]   Mintz M, Bills S, Snow R, et al. Distant supervision for relation extraction without labeled data[C]//Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2. Association for Computational Linguistics, 2009: 1003-1011.&lt;/p&gt;
&lt;p&gt;[3]   Zeng D, Liu K, Chen Y, et al. Distant supervision for relation extraction via piecewise convolutional neural networks[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015: 1753-1762.&lt;/p&gt;
&lt;p&gt;[4]   Lin Y, Shen S, Liu Z, et al. Neural relation extraction with selective attention over instances[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2016: 2124-2133.&lt;/p&gt;
&lt;p&gt;[5]   Wu Y, Bamman D, Russell S. Adversarial training for relation extraction[C]//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017: 1778-1783.&lt;/p&gt;
&lt;p&gt;[6]   Feng J, Huang M, Zhao L, et al. Reinforcement learning for relation classification from noisy data[C]//Thirty-Second AAAI Conference on Artificial Intelligence. 2018.&lt;/p&gt;
&lt;p&gt;[7]   Qin P, Weiran X U, Wang W Y. DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction[C]//Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018, 1: 496-505.&lt;/p&gt;
&lt;p&gt;[8]   Lei K, Chen D, Li Y, et al. Cooperative denoising for distantly supervised relation extraction[C]//Proceedings of the 27th International Conference on Computational Linguistics. 2018: 426-436.&lt;/p&gt;
&lt;p&gt;[9]   Vashishth S, Joshi R, Prayaga S S, et al. RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 1257-1266.&lt;/p&gt;
&lt;p&gt;[10]  Xu P, Barbosa D. Connecting Language and Knowledge with Heterogeneous Representations for Neural Relation Extraction[C]//Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019: 3201-3206.&lt;/p&gt;
&lt;p&gt;[11]Beltagy I, Lo K, Ammar W. Combining distant and direct supervision for neural relation extraction[C]//Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019: 1858-1867.&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://tmliang.github.io/tags/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/">关系抽取</category>
                                
                            
                                
                                
                                
                                    <category domain="https://tmliang.github.io/tags/%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3/">远程监督</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Text-Enhanced Representation Learning for Knowledge Graph</title>
                <link>https://tmliang.github.io/research/text-enhanced-representation-learning-for-knowledge-graph/</link>
                <guid isPermaLink="true">https://tmliang.github.io/research/text-enhanced-representation-learning-for-knowledge-graph/</guid>
                <pubDate>Wed, 18 Sep 2019 18:14:51 &#43;0800</pubDate>
                
                    <author>tm.liang@outlook.com (Liang Tianming)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</copyright>
                
                    <description>&lt;p&gt;&lt;strong&gt;Text-Enhanced Representation Learning for Knowledge Graph (IJCAI-2016)&lt;/strong&gt; &lt;a href=&#34;https://www.ijcai.org/Proceedings/16/Papers/187.pdf&#34;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;TransE、TransH、TransR&lt;/em&gt; 在 &lt;em&gt;1-N, N-1&lt;/em&gt; 和 &lt;em&gt;N-N&lt;/em&gt; 关系中的表现不佳&lt;/li&gt;
&lt;li&gt;图谱结构稀疏导致学习到的表征不准确&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;利用Word2Vec从文本中学习实体和关系的上下文表征, 然后线性变换到图谱嵌入空间中.学习线性变换的参数, 使得图谱嵌入空间中, 正确三元组的$|\widehat{\mathbf{h}}+\widehat{\mathbf{r}}-\widehat{\mathbf{t}}|_{2}^{2}$ 接近于0.&lt;/p&gt;
&lt;h3 id=&#34;input&#34;&gt;Input&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;知识图谱 $KG$, 以三元组$(h,r,t)$表示&lt;/li&gt;
&lt;li&gt;语料库 $D=\left\{w_{1}, w_{2}, \cdots, w_{m}\right\}$, $w_{i}$为单词, $m$为文本长度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h5 id=&#34;entity-annotation&#34;&gt;Entity Annotation&lt;/h5&gt;
&lt;p&gt;使用实体链接工具(AIDA, TAGME, Wikify! 等)来标注$D$中的实体. 标注后得到 ${\rm{D&#39;}}=\left\{x_{1}, x_{2}, \cdots, x_{m}\right\}$, $x_{i}$为$KG$中的实体或$D$中的单词. 其中$m&#39;&amp;lt;m$, 因为$D$中的一些词组(连续几个单词)被标为了一个实体.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h5 id=&#34;textual-context-embedding&#34;&gt;Textual Context Embedding&lt;/h5&gt;
&lt;p&gt;构建共现网络 $\mathcal{G}=(\mathcal{X}, \mathcal{Y})$ , 其中$x_{i} \in \mathcal{X}$表示单词或实体, $y_{ij} \in \mathcal{Y}$ 表示$x_{i}$和$y_{i}$的共现频率, 共现窗口设置为5.&lt;/p&gt;
&lt;p&gt;按下述步骤计算:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pointwise textual context:&lt;/strong&gt;&lt;/p&gt;
&lt;div&gt; 
$$n\left(x_{i}\right)=\left\{x_{j} | y_{i j}&gt;\theta\right\}$$
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;pairtwise textual context:&lt;/strong&gt;&lt;/p&gt;
&lt;div&gt; 
$$n\left(x_{i}, x_{j}\right)=\left\{x_{k} | x_{k} \in n\left(x_{i}\right) \cap n\left(x_{j}\right)\right\}$$
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;pointwise textual context embedding:&lt;/strong&gt;
$$\mathbf{n}\left(x_{i}\right)=\frac{1}{\sum_{x_{j} \in n\left(x_{i}\right)} y_{i j}} \sum_{x_{j} \in n\left(x_{i}\right)} y_{i j} \mathbf{x}_{j}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pairwise textual context embedding:&lt;/strong&gt;&lt;br&gt;
$${\bf{n}}\left( {{x_i},{x_j}} \right) = {1 \over {\sum\limits_{{x_k} \in n\left( {{x_i},{x_j}} \right)} {\min } \left( {{y_{ik}},{y_{jk}}} \right)}}\sum\limits_{{x_k} \in n\left( {{x_i},{x_j}} \right)} {\min } \left( {{y_{ik}},{y_{jk}}} \right){{\bf{x}}_k}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h5 id=&#34;entityrelation-representation-modeling-算法核心&#34;&gt;Entity/Relation Representation Modeling (算法核心)&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;$h$的点嵌入的线性变换&lt;/strong&gt;
$$\widehat{\mathbf{h}}=\mathbf{n}(h) \mathbf{A}+\mathbf{h}$$
&lt;strong&gt;$t$的点嵌入的线性变换&lt;/strong&gt;
$$\widehat{\mathbf{t}}=\mathbf{n}(t) \mathbf{A}+\mathbf{t}$$
&lt;strong&gt;$r$的对嵌入的线性变换&lt;/strong&gt;
$$\widehat{\mathbf{r}}=\mathbf{n}(h,t) \mathbf{B}+\mathbf{r}$$
其中A, B为权重矩阵, h,t和r是偏差.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score Fuction&lt;/strong&gt;
$$f(h, r, t)=|\widehat{\mathbf{h}}+\widehat{\mathbf{r}}-\widehat{\mathbf{t}}|_{2}^{2}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h5 id=&#34;representation-traning&#34;&gt;Representation Traning&lt;/h5&gt;
&lt;p&gt;优化目标
$$L=\sum_{(h, r, t) \in \mathcal{S}\left(h^{\prime}, r, t^{\prime}\right) \in \mathcal{S}^{\prime}} \max \left(0, f(h, r, t)+\gamma-f\left(h^{\prime}, r, t^{\prime}\right)\right)$$
其中$f(h, r, t)$表示正样本,  $f\left(h^{\prime}, r, t^{\prime}\right)$表示负样本.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-question&#34;&gt;4. Question&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;为什么能解决图谱的稀疏性问题?
&lt;ul&gt;
&lt;li&gt;利用Word2Vec从文本中学习表征, 而不是从稀疏的图谱中学习表征.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;为什么能更好处理 &lt;em&gt;1-N&lt;/em&gt;, &lt;em&gt;N-1&lt;/em&gt; 和 &lt;em&gt;N-N&lt;/em&gt; 关系?
&lt;ul&gt;
&lt;li&gt;例如在 $\widehat{\mathbf{h}}=\mathbf{n}(h) \mathbf{A}+\mathbf{h}$ 中, 对于不同的头实体, $\mathbf{A}$和$\mathbf{h}$是共享的, 因此只要$\mathbf{n}(h)$不同, $\widehat{\mathbf{h}}$ 就不会相同.&lt;/li&gt;
&lt;li&gt;当$t$固定时, 即使关系相同, 对于不同的$h$, 对嵌入$\mathbf{n}(h,t)$也不尽相同, 也就是说同一个关系会有多个表征.&lt;/li&gt;
&lt;li&gt;由以上两点可知, 在训练时, $\widehat{\mathbf{r}}$ 会不断改变, 因此不会出现TransE中相同的关系下不同头(尾)实体的表示近似相等的情况.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;同样的关系有不同的表示, 那么要使用某个关系的时候要用哪个表示?&lt;/li&gt;
&lt;li&gt;在 &lt;em&gt;1-1&lt;/em&gt; 关系上的性能不如 &lt;em&gt;TransH&lt;/em&gt; 和 &lt;em&gt;TransR&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://tmliang.github.io/tags/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA/">知识表示</category>
                                
                            
                        
                    
                
            </item>
        
    </channel>
</rss>
